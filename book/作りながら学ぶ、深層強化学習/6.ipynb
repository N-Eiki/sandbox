{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from collections import namedtuple\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\"))\n",
    "ENV = \"CartPole-v0\"\n",
    "GAMMA = 0.99\n",
    "MAX_STEPS = 200\n",
    "NUM_EPISODES = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###DDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, CAPACITY):\n",
    "        self.capacity = CAPACITY\n",
    "        self.memory = []\n",
    "        self.index = 0\n",
    "        \n",
    "    def push(self, state, action, state_next, reward):\n",
    "        if len(self.memory)<self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.index] = Transition(state, action, state_next, reward)\n",
    "        self.index = (self.index+1)% self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, n_in, n_mid, n_out):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_in, n_mid)\n",
    "        self.fc2 = nn.Linear(n_mid, n_mid)\n",
    "        self.fc3 = nn.Linear(n_mid, n_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        h2 = F.relu(self.fc2(h1))\n",
    "        out= self.fc3(h2)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "CAPACITY=10000\n",
    "\n",
    "class Brain:\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.num_actions = num_actions\n",
    "        self.memory = ReplayMemory(CAPACITY)\n",
    "        \n",
    "        n_in, n_mid, n_out = num_states, 32, num_actions\n",
    "        self.main_q_network = Net(n_in, n_mid, n_out)\n",
    "        self.target_q_network=Net(n_in, n_mid, n_out)\n",
    "        print(self.main_q_network)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.main_q_network.parameters(), lr=1e-4)\n",
    "        \n",
    "    def replay(self):\n",
    "        if len(self.memory)<BATCH_SIZE:\n",
    "            return\n",
    "        \n",
    "        self.batch, self.state_batch, self.action_batch, self.reward_batch, self.non_final_next_states = self.make_minibatch()\n",
    "        self.expected_state_action_values = self.get_expected_state_action_values()\n",
    "        self.update_main_q_network()\n",
    "        \n",
    "    def decide_action(self, state, episode):\n",
    "        epsilon = 0.5*(1/(episode+1))\n",
    "        \n",
    "        if epsilon <= np.random.uniform(0,1):\n",
    "            self.main_q_network.eval()\n",
    "            with torch.no_grad():\n",
    "                action = self.main_q_network(state).max(1)[1].view(1,1)\n",
    "        else:\n",
    "            action = torch.LongTensor([[random.randrange(self.num_actions)]])\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def make_minibatch(self):\n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "        return batch, state_batch, action_batch, reward_batch, non_final_next_states\n",
    "    \n",
    "    def get_expected_state_action_values(self):\n",
    "        self.main_q_network.eval()\n",
    "        self.target_q_network.eval()\n",
    "        \n",
    "        self.state_action_values = self.main_q_network(self.state_batch).gather(1, self.action_batch)\n",
    "        \n",
    "        non_final_mask = torch.ByteTensor(tuple(map(lambda s: s is not None, self.batch.next_state)))\n",
    "        next_state_values = torch.zeros(BATCH_SIZE)\n",
    "        a_m = torch.zeros(BATCH_SIZE).type(torch.LongTensor)\n",
    "        a_m[non_final_mask] = self.main_q_network(self.non_final_next_states).detach().max(1)[1]\n",
    "        a_m_non_final_next_states = a_m[non_final_mask].view(-1,1)\n",
    "        \n",
    "        next_state_values[non_final_mask] = self.target_q_network(self.non_final_next_states).gather(1, a_m_non_final_next_states).detach().squeeze()\n",
    "        \n",
    "        expected_state_action_values = self.reward_batch+GAMMA*next_state_values\n",
    "        return expected_state_action_values\n",
    "    \n",
    "    def update_main_q_network(self):\n",
    "        self.main_q_network.train()\n",
    "        loss = F.smooth_l1_loss(self.state_action_values, self.expected_state_action_values.unsqueeze(1))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def update_target_q_network(self):\n",
    "        self.target_q_network.load_state_dict(self.main_q_network.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.brain = Brain(num_states, num_actions)\n",
    "        \n",
    "    def update_q_function(self):\n",
    "        self.brain.replay()\n",
    "        \n",
    "    def get_action(self, state, episode):\n",
    "        action = self.brain.decide_action(state, episode)\n",
    "        return action\n",
    "    \n",
    "    def memorize(self, state, action, state_next, reward):\n",
    "        self.brain.memory.push(state, action, state_next, reward)\n",
    "        \n",
    "    def update_target_q_function(self):\n",
    "        self.brain.update_target_q_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(ENV)\n",
    "        num_states = self.env.observation_space.shape[0]\n",
    "        num_actions = self.env.action_space.n\n",
    "        self.agent = Agent(num_states, num_actions)\n",
    "        \n",
    "    def run(self):\n",
    "        episode_10_list = np.zeros(10)\n",
    "        complete_episodes=0\n",
    "        episode_final=False\n",
    "        \n",
    "        for episode in range(NUM_EPISODES):\n",
    "            observation = self.env.reset()\n",
    "            state = observation\n",
    "            state = torch.from_numpy(state).type(torch.FloatTensor)\n",
    "            state = torch.unsqueeze(state, 0)\n",
    "            \n",
    "            for step in range(MAX_STEPS):\n",
    "                action = self.agent.get_action(state, episode)\n",
    "                observation_next, _, done, _ = self.env.step(action.item())\n",
    "                \n",
    "                if done:\n",
    "                    state_next = None\n",
    "                    episode_10_list = np.hstack((episode_10_list[1:], step+1))\n",
    "                    \n",
    "                    if step<195:\n",
    "                        reward = torch.FloatTensor([-1.0])\n",
    "                        complete_episodes=0\n",
    "                    else:\n",
    "                        reward = torch.FloatTensor([1.0])\n",
    "                        complete_episodes = complete_episodes+1\n",
    "                        \n",
    "                else:\n",
    "                    reward = torch.Tensor([0.0])\n",
    "                    state_next = observation_next\n",
    "                    state_next = torch.from_numpy(state_next).type(torch.FloatTensor)\n",
    "                    state_next = torch.unsqueeze(state_next, 0)\n",
    "                    \n",
    "                self.agent.memorize(state, action, state_next, reward)\n",
    "                self.agent.update_q_function()\n",
    "                state = state_next\n",
    "                \n",
    "                if done:\n",
    "                    print(f'{episode} episode finished after {step+1} steps , average of 10 try {episode_10_list.mean()}')\n",
    "                    if episode%2 == 0:\n",
    "                        self.agent.update_target_q_function()\n",
    "                    break\n",
    "                    \n",
    "            if episode_final:\n",
    "                self.env.close()\n",
    "                break\n",
    "                \n",
    "            if complete_episodes>=10:\n",
    "                    print('seikou')\n",
    "                    episode_final=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cartpole_env = Environment()\n",
    "# cartpole_env.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##dueling network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, n_in, n_mid, n_out):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_in, n_mid)\n",
    "        self.fc2 = nn.Linear(n_mid, n_mid)\n",
    "        self.fc3_adv = nn.Linear(n_mid, n_out)\n",
    "        self.fc3_v   = nn.Linear(n_mid,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = F.relu(x)\n",
    "        h2 = F.relu(h1)\n",
    "        \n",
    "        adv = self.fc3_adv(h2)\n",
    "        val = self.fc3_v(h2).expand(-1, adv.size(1))\n",
    "        \n",
    "        out = val + adv -adv.mean(1, keepdim=True).expand(-1, adv.size(1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prioritized experience replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "TD_ERROR_EPSILON=0.0001\n",
    "\n",
    "class TDerrorMemory:\n",
    "    def __init__(self, CAPACITY):\n",
    "        self.capacity = CAPACITY\n",
    "        self.memory = []\n",
    "        self.index = 0\n",
    "        \n",
    "    def push(self, td_error):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        \n",
    "        self.memory[self.index] = self.td_error\n",
    "        self.index = (self.index+1)%self.capacity\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    def get_prioritized_indexes(self, batch_size):\n",
    "        sum_absolute_td_error = np.sum(np.absolute(self.memory))\n",
    "        sum_absolute_td_error += TD_ERROR_EPSILON*len(self.memory)\n",
    "        \n",
    "        rand_list = np.randomuniform(0, sum_absolute_td_error, batch_size)\n",
    "        rand_list = np.sort(rand_list)\n",
    "        \n",
    "        indexes = []\n",
    "        idx = 0\n",
    "        tmp_sum_absolute_td_error = 0\n",
    "        for randnum in rand_list:\n",
    "            while tmp_sum_absolute_td_error<randnum:\n",
    "                tmp_sum_absolute_td_error +=(abs(self.memory[idx]) + TD_ERROR_EPSILON)\n",
    "                idx +=1\n",
    "                \n",
    "            if idx >= len(self.memory):\n",
    "                idx = len(self.memory)-1\n",
    "            indexes.append(idx)\n",
    "            \n",
    "        return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloustStorage(object):\n",
    "    def __init__(self, num_steps, num_processes, obs_shape):\n",
    "        self.observations = torch.zeros(num_steps+1, num_processes, 4)\n",
    "        self.masks = torch.ones(num_steps+1, num_processes, 1)\n",
    "        self.rewards = torch.zeros(num_steps, num_processes,1)\n",
    "        self.actions = torch.zeros(num_steps, num_processes, 1).long()\n",
    "        \n",
    "        self.returns = torch.zeros(num_steps, 1, num_processes, 1)\n",
    "        self.index = 0\n",
    "        \n",
    "    def insert(self, current_obs, action, reward, mask):\n",
    "        self.observations[self.index+1].copy_(current_obs)\n",
    "        self.masks[self.index+1].copy_(mask)\n",
    "        self.rewards[self.index].copy_(reward)\n",
    "        self.actions[self.index].copy_(action)\n",
    "        \n",
    "        self.index = (self.index+1)%CAPACITY\n",
    "        \n",
    "    def after_update(self):\n",
    "        self.observations[0].copy_(self.observations[-1])\n",
    "        self.masks[0].copy_(self.masks[-1])\n",
    "        \n",
    "    def compute_returns(self, next_values):\n",
    "        self.returns[-1] = next_value\n",
    "        for ad_step in reversed(range(self.rewards.size(0))):\n",
    "            self.returns[ad_step] = self.returns[ad_step+1]* GAMMA*self.masks[ad_step+1]+self.rewards[ad_step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, n_in, n_mid, n_out):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_in, n_mid)\n",
    "        self.fc2 = nn.Linear(n_mid, n_mid)\n",
    "        self.fc3 = nn.Linear(n_mid, n_out)\n",
    "        self.actor = nn.Linear(n_mid, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        h2 = F.relu(self.fc2(h1))\n",
    "        critic_out = self.critic(h2)\n",
    "        actor_out = self.actor(h2)\n",
    "        \n",
    "        return critic_out, actor_out\n",
    "    \n",
    "    def act(self, x):\n",
    "        value, actor_out = self(x)\n",
    "        action_probs = F.softmax(actor_out, dim=1)\n",
    "        action = actin_probs.multinomial(num_samples=1)\n",
    "        return action\n",
    "    \n",
    "    def get_values(self, x):\n",
    "        value, actor_output = self(x)\n",
    "        return value\n",
    "    \n",
    "    def evaluate_actions(self, x, acitons):\n",
    "        value, actor_out = self(x)\n",
    "        log_probs = F.log_softmax(actor_out, dim=1)\n",
    "        action_log_probs = log_probs.gather(1, actions)\n",
    "        \n",
    "        probs = F.softmax(actor_out, dim=1)\n",
    "        entropy = -(log_probs*probs).sum(-1).mean()\n",
    "        return value, aciton_log_probs, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
