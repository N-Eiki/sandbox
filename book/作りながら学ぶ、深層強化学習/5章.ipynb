{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "%matplotlib inline\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr(name_a='Adesu', value_b=100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tr = namedtuple('tr', (\"name_a\", \"value_b\"))\n",
    "Tr_object = Tr(\"Adesu\", 100)\n",
    "print(Tr_object)\n",
    "Tr_object.value_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\n",
    "    \"Transition\", ('state', \"action\", \"next_state\", \"reward\")\n",
    ")\n",
    "\n",
    "ENV = \"CartPole-v0\"\n",
    "GAMMA = 0.99\n",
    "MAX_STEPS=200\n",
    "NUM_EPISODES=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, CAPACITY):\n",
    "        self.capacity = CAPACITY\n",
    "        self.memory = []\n",
    "        self.index = 0\n",
    "        \n",
    "    def push(self, state, action, state_next, reward):\n",
    "        if len(self.memory)<self.capacity:\n",
    "            self.memory.append(None)\n",
    "            \n",
    "        self.memory[self.index] = Transition(state, action, state_next, reward)\n",
    "        self.index = (self.index+1)%self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "BATCH_SIZE=32\n",
    "CAPACITY=10000\n",
    "\n",
    "class Brain:\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.num_actions = num_actions\n",
    "        self.memory = ReplayMemory(CAPACITY)\n",
    "        \n",
    "#         self.model = {\n",
    "#             nn.Linear(num_states, 32),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(32, 32),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(32, num_actions)\n",
    "#         }\n",
    "        self.model = nn.Sequential()\n",
    "        self.model.add_module('fc1', nn.Linear(num_states, 32))\n",
    "        self.model.add_module('relu1', nn.ReLU())\n",
    "        self.model.add_module('fc2', nn.Linear(32, 32))\n",
    "        self.model.add_module('relu2', nn.ReLU())\n",
    "        self.model.add_module('fc3', nn.Linear(32, num_actions))\n",
    "        print(self.model)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-4)\n",
    "    \n",
    "    def replay(self):\n",
    "        if len(self.memory)<BATCH_SIZE:\n",
    "            return \n",
    "            \n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch= torch.cat(batch.action)\n",
    "        \n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "        #Q(s,t)を決定する\n",
    "        self.model.eval()\n",
    "        state_action_values = self.model(state_batch).gather(1, action_batch)\n",
    "        non_final_mask = torch.ByteTensor(tuple(map(lambda s : s is not None, batch.next_state)))\n",
    "        next_state_values = torch.zeros(BATCH_SIZE)\n",
    "        next_state_values[non_final_mask] = self.model(non_final_next_states).max(1)[0].detach()\n",
    "        expected_state_action_values = reward_batch + GAMMA*next_state_values\n",
    "        \n",
    "        self.model.train()\n",
    "        #損失関数\n",
    "        loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def decide_action(self, state, episode):\n",
    "        epsilon = 0.5 * (1/(episode+1))\n",
    "        \n",
    "        if epsilon < np.random.uniform(0,1):\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                action = self.model(state).max(1)[1].view(1,1)\n",
    "        else:\n",
    "            action = torch.LongTensor([[random.randrange(self.num_actions)]])\n",
    "            \n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.brain = Brain(num_states, num_actions)\n",
    "    \n",
    "    def update_q_function(self):\n",
    "        self.brain.replay()\n",
    "    \n",
    "    def get_action(self, state, episode):\n",
    "        action = self.brain.decide_action(state, episode)\n",
    "        return action\n",
    "    \n",
    "    def memorize(self, state, action, state_next, reward):\n",
    "        self.brain.memory.push(state, action, state_next, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(ENV)\n",
    "        num_states = self.env.observation_space.shape[0]\n",
    "        num_actions = self.env.action_space.n\n",
    "        self.agent = Agent(num_states, num_actions)\n",
    "        \n",
    "    def run(self):\n",
    "        episode_10_list = np.zeros(10)\n",
    "        complete_episodes = 0\n",
    "        episode_final = False\n",
    "        frames = []\n",
    "        \n",
    "        for episode in range(NUM_EPISODES):\n",
    "            observation = self.env.reset()\n",
    "            state = observation\n",
    "            state = torch.from_numpy(state).type(torch.FloatTensor)\n",
    "            state = torch.unsqueeze(state, 0)\n",
    "            \n",
    "            for step in range(MAX_STEPS):\n",
    "                if episode_final:\n",
    "                    frames.append(self.env.render(mode=\"rgb_array\"))\n",
    "                action = self.agent.get_action(state, episode)\n",
    "                \n",
    "                observation_next, _, done, _ = self.env.step(action.item())\n",
    "                \n",
    "                if done:\n",
    "                    state_next = None\n",
    "                    episode_10_list = np.hstack((episode_10_list[1:], step+1))\n",
    "                    if step <195:\n",
    "                        reward = torch.FloatTensor([-1.0])\n",
    "                        complete_episodes=0\n",
    "                    else:\n",
    "                        reward = torch.FloatTensor([1.0])\n",
    "                        complete_episodes=complete_episodes+1\n",
    "                        \n",
    "                else:\n",
    "                    reward = torch.FloatTensor([0.0])\n",
    "                    state_next = observation_next\n",
    "                    state_next = torch.from_numpy(state_next).type(torch.FloatTensor)\n",
    "                    state_next = torch.unsqueeze(state_next, 0)\n",
    "                self.agent.memorize(state, action, state_next, reward)\n",
    "                self.agent.update_q_function()\n",
    "                state = state_next\n",
    "                \n",
    "                    \n",
    "                if done:\n",
    "                        print(f'{episode} episode, finished after {step+1} steps mean of 10 try {episode_10_list.mean()}')\n",
    "                        break\n",
    "                        \n",
    "            if episode_final:\n",
    "                break\n",
    "            if complete_episodes>=10:\n",
    "                print('10 renzoku')\n",
    "                \n",
    "                episode_final=True\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (fc1): Linear(in_features=4, out_features=32, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (fc3): Linear(in_features=32, out_features=2, bias=True)\n",
      ")\n",
      "0 episode, finished after 10 steps mean of 10 try 1.0\n",
      "1 episode, finished after 10 steps mean of 10 try 2.0\n",
      "2 episode, finished after 9 steps mean of 10 try 2.9\n",
      "3 episode, finished after 10 steps mean of 10 try 3.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nagataeiki/.pyenv/versions/3.7.4/lib/python3.7/site-packages/ipykernel_launcher.py:47: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/IndexingUtils.h:20.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 episode, finished after 9 steps mean of 10 try 4.8\n",
      "5 episode, finished after 11 steps mean of 10 try 5.9\n",
      "6 episode, finished after 10 steps mean of 10 try 6.9\n",
      "7 episode, finished after 9 steps mean of 10 try 7.8\n",
      "8 episode, finished after 9 steps mean of 10 try 8.7\n",
      "9 episode, finished after 8 steps mean of 10 try 9.5\n",
      "10 episode, finished after 10 steps mean of 10 try 9.5\n",
      "11 episode, finished after 10 steps mean of 10 try 9.5\n",
      "12 episode, finished after 10 steps mean of 10 try 9.6\n",
      "13 episode, finished after 8 steps mean of 10 try 9.4\n",
      "14 episode, finished after 10 steps mean of 10 try 9.5\n",
      "15 episode, finished after 11 steps mean of 10 try 9.5\n",
      "16 episode, finished after 11 steps mean of 10 try 9.6\n",
      "17 episode, finished after 10 steps mean of 10 try 9.7\n",
      "18 episode, finished after 9 steps mean of 10 try 9.7\n",
      "19 episode, finished after 10 steps mean of 10 try 9.9\n",
      "20 episode, finished after 11 steps mean of 10 try 10.0\n",
      "21 episode, finished after 10 steps mean of 10 try 10.0\n",
      "22 episode, finished after 14 steps mean of 10 try 10.4\n",
      "23 episode, finished after 17 steps mean of 10 try 11.3\n",
      "24 episode, finished after 15 steps mean of 10 try 11.8\n",
      "25 episode, finished after 18 steps mean of 10 try 12.5\n",
      "26 episode, finished after 41 steps mean of 10 try 15.5\n",
      "27 episode, finished after 125 steps mean of 10 try 27.0\n",
      "28 episode, finished after 12 steps mean of 10 try 27.3\n",
      "29 episode, finished after 52 steps mean of 10 try 31.5\n",
      "30 episode, finished after 119 steps mean of 10 try 42.3\n",
      "31 episode, finished after 8 steps mean of 10 try 42.1\n",
      "32 episode, finished after 9 steps mean of 10 try 41.6\n",
      "33 episode, finished after 8 steps mean of 10 try 40.7\n",
      "34 episode, finished after 9 steps mean of 10 try 40.1\n",
      "35 episode, finished after 9 steps mean of 10 try 39.2\n",
      "36 episode, finished after 10 steps mean of 10 try 36.1\n",
      "37 episode, finished after 11 steps mean of 10 try 24.7\n",
      "38 episode, finished after 9 steps mean of 10 try 24.4\n",
      "39 episode, finished after 10 steps mean of 10 try 20.2\n",
      "40 episode, finished after 25 steps mean of 10 try 10.8\n",
      "41 episode, finished after 44 steps mean of 10 try 14.4\n",
      "42 episode, finished after 30 steps mean of 10 try 16.5\n",
      "43 episode, finished after 36 steps mean of 10 try 19.3\n",
      "44 episode, finished after 41 steps mean of 10 try 22.5\n",
      "45 episode, finished after 35 steps mean of 10 try 25.1\n",
      "46 episode, finished after 26 steps mean of 10 try 26.7\n",
      "47 episode, finished after 26 steps mean of 10 try 28.2\n",
      "48 episode, finished after 32 steps mean of 10 try 30.5\n",
      "49 episode, finished after 50 steps mean of 10 try 34.5\n",
      "50 episode, finished after 37 steps mean of 10 try 35.7\n",
      "51 episode, finished after 18 steps mean of 10 try 33.1\n",
      "52 episode, finished after 21 steps mean of 10 try 32.2\n",
      "53 episode, finished after 13 steps mean of 10 try 29.9\n",
      "54 episode, finished after 25 steps mean of 10 try 28.3\n",
      "55 episode, finished after 49 steps mean of 10 try 29.7\n",
      "56 episode, finished after 15 steps mean of 10 try 28.6\n",
      "57 episode, finished after 15 steps mean of 10 try 27.5\n",
      "58 episode, finished after 13 steps mean of 10 try 25.6\n",
      "59 episode, finished after 13 steps mean of 10 try 21.9\n",
      "60 episode, finished after 15 steps mean of 10 try 19.7\n",
      "61 episode, finished after 15 steps mean of 10 try 19.4\n",
      "62 episode, finished after 15 steps mean of 10 try 18.8\n",
      "63 episode, finished after 13 steps mean of 10 try 18.8\n",
      "64 episode, finished after 14 steps mean of 10 try 17.7\n",
      "65 episode, finished after 18 steps mean of 10 try 14.6\n",
      "66 episode, finished after 22 steps mean of 10 try 15.3\n",
      "67 episode, finished after 22 steps mean of 10 try 16.0\n",
      "68 episode, finished after 23 steps mean of 10 try 17.0\n",
      "69 episode, finished after 20 steps mean of 10 try 17.7\n",
      "70 episode, finished after 13 steps mean of 10 try 17.5\n",
      "71 episode, finished after 24 steps mean of 10 try 18.4\n",
      "72 episode, finished after 31 steps mean of 10 try 20.0\n",
      "73 episode, finished after 19 steps mean of 10 try 20.6\n",
      "74 episode, finished after 19 steps mean of 10 try 21.1\n",
      "75 episode, finished after 16 steps mean of 10 try 20.9\n",
      "76 episode, finished after 26 steps mean of 10 try 21.3\n",
      "77 episode, finished after 32 steps mean of 10 try 22.3\n",
      "78 episode, finished after 37 steps mean of 10 try 23.7\n",
      "79 episode, finished after 59 steps mean of 10 try 27.6\n",
      "80 episode, finished after 33 steps mean of 10 try 29.6\n",
      "81 episode, finished after 39 steps mean of 10 try 31.1\n",
      "82 episode, finished after 40 steps mean of 10 try 32.0\n",
      "83 episode, finished after 43 steps mean of 10 try 34.4\n",
      "84 episode, finished after 129 steps mean of 10 try 45.4\n",
      "85 episode, finished after 53 steps mean of 10 try 49.1\n",
      "86 episode, finished after 32 steps mean of 10 try 49.7\n",
      "87 episode, finished after 54 steps mean of 10 try 51.9\n",
      "88 episode, finished after 63 steps mean of 10 try 54.5\n",
      "89 episode, finished after 84 steps mean of 10 try 57.0\n",
      "90 episode, finished after 59 steps mean of 10 try 59.6\n",
      "91 episode, finished after 76 steps mean of 10 try 63.3\n",
      "92 episode, finished after 82 steps mean of 10 try 67.5\n",
      "93 episode, finished after 60 steps mean of 10 try 69.2\n",
      "94 episode, finished after 75 steps mean of 10 try 63.8\n",
      "95 episode, finished after 161 steps mean of 10 try 74.6\n",
      "96 episode, finished after 81 steps mean of 10 try 79.5\n",
      "97 episode, finished after 200 steps mean of 10 try 94.1\n",
      "98 episode, finished after 140 steps mean of 10 try 101.8\n",
      "99 episode, finished after 175 steps mean of 10 try 110.9\n",
      "100 episode, finished after 168 steps mean of 10 try 121.8\n",
      "101 episode, finished after 79 steps mean of 10 try 122.1\n",
      "102 episode, finished after 132 steps mean of 10 try 127.1\n",
      "103 episode, finished after 194 steps mean of 10 try 140.5\n",
      "104 episode, finished after 200 steps mean of 10 try 153.0\n",
      "105 episode, finished after 168 steps mean of 10 try 153.7\n",
      "106 episode, finished after 200 steps mean of 10 try 165.6\n",
      "107 episode, finished after 200 steps mean of 10 try 165.6\n",
      "108 episode, finished after 193 steps mean of 10 try 170.9\n",
      "109 episode, finished after 200 steps mean of 10 try 173.4\n",
      "110 episode, finished after 200 steps mean of 10 try 176.6\n",
      "111 episode, finished after 200 steps mean of 10 try 188.7\n",
      "112 episode, finished after 200 steps mean of 10 try 195.5\n",
      "113 episode, finished after 200 steps mean of 10 try 196.1\n",
      "114 episode, finished after 200 steps mean of 10 try 196.1\n",
      "115 episode, finished after 200 steps mean of 10 try 199.3\n",
      "116 episode, finished after 200 steps mean of 10 try 199.3\n",
      "117 episode, finished after 200 steps mean of 10 try 199.3\n",
      "118 episode, finished after 200 steps mean of 10 try 200.0\n",
      "10 renzoku\n",
      "119 episode, finished after 200 steps mean of 10 try 200.0\n"
     ]
    }
   ],
   "source": [
    "cartpole_env = Environment()\n",
    "cartpole_env.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
