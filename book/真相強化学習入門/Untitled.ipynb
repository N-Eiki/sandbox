{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nagataeiki/opt/anaconda3/envs/learning/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for _ in range(100):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chainerでORを学習\n",
    "\n",
    "import numpy as np\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import chainer.initializer as I\n",
    "from chainer import training\n",
    "from chainer.training import extensions\n",
    "\n",
    "    \n",
    "class MyChain(chainer.Chain):\n",
    "    def __init__(self):\n",
    "        super(MyChain, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.l1 = L.Linear(2, 3)#入力2，中間層3\n",
    "            self.l2 = L.Linear(3, 2)#中間層3，出力2\n",
    "    def __call__(self, x):\n",
    "        h1 = F.relu(self.l1(x))#ReLU関数\n",
    "        y = self.l2(h1)\n",
    "        return y        \n",
    "\n",
    "epoch = 100\n",
    "batchsize=4\n",
    "\n",
    "#データの作成\n",
    "trainx = np.array(([0,0], [0,1], [1,0], [1,1]), dtype=np.float32)\n",
    "trainy = np.array([0,1,1,1], dtype=np.int32)\n",
    "train = chainer.datasets.TupleDataset(trainx, trainy)\n",
    "test = chainer.datasets.TupleDataset(trainx, trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch       main/loss   validation/main/loss  main/accuracy  validation/main/accuracy  elapsed_time\n",
      "\u001b[J1           0.752208    0.750766              0.25           0                         0.00500888    \n",
      "\u001b[J2           0.750766    0.749332              0              0                         0.0100288     \n",
      "\u001b[J3           0.749332    0.747906              0              0                         0.0158575     \n",
      "\u001b[J4           0.747906    0.746488              0              0                         0.0249481     \n",
      "\u001b[J5           0.746488    0.745077              0              0                         0.0324432     \n",
      "\u001b[J6           0.745077    0.743675              0              0                         0.0387258     \n",
      "\u001b[J7           0.743675    0.74228               0              0                         0.0520108     \n",
      "\u001b[J8           0.74228     0.740894              0              0                         0.0585121     \n",
      "\u001b[J9           0.740894    0.739516              0              0                         0.0640513     \n",
      "\u001b[J10          0.739516    0.738146              0              0                         0.070838      \n",
      "\u001b[J11          0.738146    0.736785              0              0                         0.0770268     \n",
      "\u001b[J12          0.736785    0.735432              0              0                         0.0814212     \n",
      "\u001b[J13          0.735432    0.734087              0              0                         0.0864298     \n",
      "\u001b[J14          0.734087    0.732751              0              0                         0.0910584     \n",
      "\u001b[J15          0.732751    0.731424              0              0                         0.0962167     \n",
      "\u001b[J16          0.731424    0.730105              0              0                         0.100756      \n",
      "\u001b[J17          0.730105    0.728795              0              0                         0.105772      \n",
      "\u001b[J18          0.728795    0.727494              0              0                         0.111905      \n",
      "\u001b[J19          0.727494    0.726201              0              0                         0.117544      \n",
      "\u001b[J20          0.726201    0.724917              0              0                         0.121888      \n",
      "\u001b[J21          0.724917    0.723642              0              0                         0.127671      \n",
      "\u001b[J22          0.723642    0.722375              0              0                         0.132642      \n",
      "\u001b[J23          0.722375    0.721118              0              0                         0.138918      \n",
      "\u001b[J24          0.721118    0.719869              0              0                         0.144265      \n",
      "\u001b[J25          0.719869    0.718629              0              0.25                      0.149385      \n",
      "\u001b[J26          0.718629    0.717397              0.25           0.25                      0.154984      \n",
      "\u001b[J27          0.717397    0.716175              0.25           0.25                      0.160378      \n",
      "\u001b[J28          0.716175    0.714961              0.25           0.25                      0.166228      \n",
      "\u001b[J29          0.714961    0.713755              0.25           0.25                      0.170885      \n",
      "\u001b[J30          0.713755    0.712558              0.25           0.25                      0.17604       \n",
      "\u001b[J31          0.712558    0.71137               0.25           0.25                      0.180884      \n",
      "\u001b[J32          0.71137     0.710191              0.25           0.25                      0.186444      \n",
      "\u001b[J33          0.710191    0.70902               0.25           0.25                      0.192043      \n",
      "\u001b[J34          0.70902     0.707857              0.25           0.25                      0.197069      \n",
      "\u001b[J35          0.707857    0.706703              0.25           0.25                      0.205611      \n",
      "\u001b[J36          0.706703    0.705557              0.25           0.25                      0.21258       \n",
      "\u001b[J37          0.705557    0.704419              0.25           0.25                      0.218491      \n",
      "\u001b[J38          0.704419    0.70329               0.25           0.25                      0.226992      \n",
      "\u001b[J39          0.70329     0.702169              0.25           0.5                       0.232613      \n",
      "\u001b[J40          0.702169    0.701056              0.5            0.5                       0.237399      \n",
      "\u001b[J41          0.701056    0.699951              0.5            0.5                       0.24485       \n",
      "\u001b[J42          0.699951    0.698854              0.5            0.5                       0.249909      \n",
      "\u001b[J43          0.698854    0.697765              0.5            0.5                       0.257871      \n",
      "\u001b[J44          0.697765    0.696684              0.5            0.5                       0.266032      \n",
      "\u001b[J45          0.696684    0.69561               0.5            0.5                       0.271115      \n",
      "\u001b[J46          0.69561     0.694544              0.5            0.5                       0.2785        \n",
      "\u001b[J47          0.694544    0.693486              0.5            0.5                       0.284416      \n",
      "\u001b[J48          0.693486    0.692436              0.5            0.75                      0.291354      \n",
      "\u001b[J49          0.692436    0.691393              0.75           0.75                      0.297096      \n",
      "\u001b[J50          0.691393    0.690357              0.75           0.75                      0.303175      \n",
      "\u001b[J51          0.690357    0.689328              0.75           0.75                      0.311583      \n",
      "\u001b[J52          0.689328    0.688307              0.75           0.75                      0.317564      \n",
      "\u001b[J53          0.688307    0.687293              0.75           0.75                      0.322769      \n",
      "\u001b[J54          0.687293    0.686285              0.75           0.75                      0.328672      \n",
      "\u001b[J55          0.686285    0.685285              0.75           0.75                      0.335012      \n",
      "\u001b[J56          0.685285    0.684292              0.75           0.75                      0.341317      \n",
      "\u001b[J57          0.684292    0.683305              0.75           0.75                      0.346336      \n",
      "\u001b[J58          0.683305    0.682325              0.75           0.75                      0.353607      \n",
      "\u001b[J59          0.682325    0.681352              0.75           0.75                      0.35928       \n",
      "\u001b[J60          0.681352    0.680385              0.75           0.75                      0.364439      \n",
      "\u001b[J61          0.680385    0.679424              0.75           0.75                      0.371036      \n",
      "\u001b[J62          0.679424    0.67847               0.75           0.75                      0.377526      \n",
      "\u001b[J63          0.67847     0.677522              0.75           0.75                      0.383233      \n",
      "\u001b[J64          0.677522    0.676581              0.75           0.75                      0.39096       \n",
      "\u001b[J65          0.676581    0.675645              0.75           0.75                      0.396507      \n",
      "\u001b[J66          0.675645    0.674715              0.75           0.75                      0.40362       \n",
      "\u001b[J67          0.674715    0.673791              0.75           0.75                      0.409372      \n",
      "\u001b[J68          0.673791    0.672873              0.75           0.75                      0.416218      \n",
      "\u001b[J69          0.672873    0.67196               0.75           0.75                      0.423912      \n",
      "\u001b[J70          0.67196     0.671053              0.75           0.75                      0.431488      \n",
      "\u001b[J71          0.671054    0.670152              0.75           0.75                      0.437691      \n",
      "\u001b[J72          0.670152    0.669256              0.75           0.75                      0.445618      \n",
      "\u001b[J73          0.669256    0.668365              0.75           0.75                      0.45371       \n",
      "\u001b[J74          0.668365    0.66748               0.75           0.75                      0.460084      \n",
      "\u001b[J75          0.66748     0.666599              0.75           0.75                      0.466026      \n",
      "\u001b[J76          0.666599    0.665724              0.75           0.75                      0.472217      \n",
      "\u001b[J77          0.665724    0.664853              0.75           0.75                      0.4784        \n",
      "\u001b[J78          0.664853    0.663988              0.75           0.75                      0.485534      \n",
      "\u001b[J79          0.663988    0.663127              0.75           0.75                      0.491564      \n",
      "\u001b[J80          0.663127    0.66227               0.75           0.75                      0.498763      \n",
      "\u001b[J81          0.66227     0.661419              0.75           0.75                      0.504436      \n",
      "\u001b[J82          0.661419    0.660571              0.75           0.75                      0.511288      \n",
      "\u001b[J83          0.660571    0.659728              0.75           0.75                      0.517327      \n",
      "\u001b[J84          0.659728    0.65889               0.75           0.75                      0.524213      \n",
      "\u001b[J85          0.65889     0.658055              0.75           0.75                      0.530405      \n",
      "\u001b[J86          0.658055    0.657225              0.75           0.75                      0.538111      \n",
      "\u001b[J87          0.657225    0.656398              0.75           0.75                      0.544573      \n",
      "\u001b[J88          0.656398    0.655576              0.75           0.75                      0.551333      \n",
      "\u001b[J89          0.655576    0.654757              0.75           0.75                      0.558168      \n",
      "\u001b[J90          0.654757    0.653942              0.75           0.75                      0.564904      \n",
      "\u001b[J91          0.653942    0.65313               0.75           0.75                      0.571303      \n",
      "\u001b[J92          0.65313     0.652322              0.75           0.75                      0.578393      \n",
      "\u001b[J93          0.652322    0.651517              0.75           0.75                      0.584914      \n",
      "\u001b[J94          0.651517    0.650716              0.75           0.75                      0.591676      \n",
      "\u001b[J95          0.650716    0.649918              0.75           0.75                      0.597963      \n",
      "\u001b[J96          0.649918    0.649123              0.75           0.75                      0.605156      \n",
      "\u001b[J97          0.649123    0.648331              0.75           0.75                      0.611329      \n",
      "\u001b[J98          0.648331    0.647542              0.75           0.75                      0.619335      \n",
      "\u001b[J99          0.647542    0.646756              0.75           0.75                      0.627525      \n",
      "\u001b[J100         0.646756    0.645972              0.75           0.75                      0.635009      \n"
     ]
    }
   ],
   "source": [
    "# ニューラルネットワークの登録\n",
    "model = L.Classifier(MyChain(), lossfun=F.softmax_cross_entropy)\n",
    "#chainer.serializers.load_npz(\"result/out.model\", model)\n",
    "optimizer = chainer.optimizers.Adam()\n",
    "optimizer.setup(model)\n",
    "\n",
    "# イテレータの定義\n",
    "train_iter = chainer.iterators.SerialIterator(train, batchsize)# 学習用\n",
    "test_iter = chainer.iterators.SerialIterator(test, batchsize, repeat=False, shuffle=False)# 評価用\n",
    "# アップデータの登録\n",
    "updater = training.StandardUpdater(train_iter, optimizer)\n",
    "\n",
    "# トレーナーの登録\n",
    "trainer = training.Trainer(updater, (epoch, 'epoch'))\n",
    "\n",
    "# 学習状況の表示や保存\n",
    "trainer.extend(extensions.LogReport())#ログ\n",
    "trainer.extend(extensions.Evaluator(test_iter, model))# エポック数の表示\n",
    "trainer.extend(extensions.PrintReport(['epoch', 'main/loss', 'validation/main/loss','main/accuracy', 'validation/main/accuracy', 'elapsed_time'] ))#計算状態の表示\n",
    "#trainer.extend(extensions.dump_graph('main/loss'))#ニューラルネットワークの構造\n",
    "#trainer.extend(extensions.PlotReport(['main/loss', 'validation/main/loss'], 'epoch',file_name='loss.png'))#誤差のグラフ\n",
    "#trainer.extend(extensions.PlotReport(['main/accuracy', 'validation/main/accuracy'],'epoch', file_name='accuracy.png'))#精度のグラフ\n",
    "#trainer.extend(extensions.snapshot(), trigger=(100, 'epoch'))# 学習再開のためのスナップショット出力\n",
    "#chainer.serializers.load_npz(\"result/snapshot_iter_500\", trainer)#再開用\n",
    "#chainer.serializers.save_npz(\"result/out.model\", model)\n",
    "\n",
    "# 学習開始\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "class MyChain(chainer.Chain):\n",
    "    def __init__(self):\n",
    "        super(MyChain, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.l1 = L.Linear(64, 100)\n",
    "            self.l2 = L.Linear(100, 100)\n",
    "            self.l3 = L.Linear(100, 10)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        h1=F.relu(self.l1(x))\n",
    "        h2=F.relu(self.l2(h1))\n",
    "        y = self.l3(h2)\n",
    "        return y\n",
    "epoch=20\n",
    "batchsize=100\n",
    "\n",
    "digits = load_digits()\n",
    "data_train, data_test, label_train ,label_test = train_test_split(digits.data, digits.target, test_size=0.2)\n",
    "data_train = data_train.astype(np.float32)\n",
    "data_test = data_test.astype(np.float32)\n",
    "train = chainer.datasets.TupleDataset(data_train, label_train)\n",
    "test = chainer.datasets.TupleDataset(data_test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch       main/loss   validation/main/loss  main/accuracy  validation/main/accuracy  elapsed_time\n",
      "\u001b[J1           3.00107     1.35255               0.236667       0.553333                  0.0930358     \n",
      "\u001b[J2           0.811325    0.541528              0.755714       0.8175                    0.162832      \n",
      "\u001b[J3           0.363834    0.348152              0.894          0.906667                  0.261845      \n",
      "\u001b[J4           0.225618    0.279121              0.94           0.920833                  0.339103      \n",
      "\u001b[J5           0.167101    0.243722              0.953571       0.930833                  0.404189      \n",
      "\u001b[J6           0.133985    0.204815              0.961333       0.9425                    0.467817      \n",
      "\u001b[J7           0.104447    0.184905              0.97           0.949167                  0.532126      \n",
      "\u001b[J8           0.0862895   0.165434              0.976429       0.953333                  0.59962       \n",
      "\u001b[J9           0.0698766   0.157184              0.987333       0.9675                    0.681871      \n",
      "\u001b[J10          0.0570214   0.14475               0.99           0.965                     0.766079      \n",
      "\u001b[J11          0.0482141   0.131313              0.994          0.9725                    0.840405      \n",
      "\u001b[J12          0.0406773   0.126503              0.995714       0.9725                    0.90335       \n",
      "\u001b[J13          0.0344515   0.121051              0.997143       0.9775                    0.960472      \n",
      "\u001b[J14          0.0289581   0.110104              0.997333       0.9725                    1.0208        \n",
      "\u001b[J15          0.0246993   0.120913              0.998571       0.975                     1.07533       \n",
      "\u001b[J16          0.0224362   0.108166              0.997857       0.9775                    1.14002       \n",
      "\u001b[J17          0.0188986   0.107695              0.998667       0.9725                    1.20336       \n",
      "\u001b[J18          0.0167939   0.103061              0.997857       0.9775                    1.27051       \n",
      "\u001b[J19          0.0149569   0.111181              0.998667       0.9725                    1.33865       \n",
      "\u001b[J20          0.0135246   0.0978745             0.999286       0.98                      1.41333       \n"
     ]
    }
   ],
   "source": [
    "model = L.Classifier(MyChain(), lossfun=F.softmax_cross_entropy)\n",
    "#chainer.serializers.load_npz(\"result/out.model\", model)\n",
    "optimizer = chainer.optimizers.Adam()\n",
    "optimizer.setup(model)\n",
    "\n",
    "# イテレータの定義\n",
    "train_iter = chainer.iterators.SerialIterator(train, batchsize)# 学習用\n",
    "test_iter = chainer.iterators.SerialIterator(test, batchsize, repeat=False, shuffle=False)# 評価用\n",
    "\n",
    "# アップデータの登録\n",
    "updater = training.StandardUpdater(train_iter, optimizer)\n",
    "\n",
    "# トレーナーの登録\n",
    "trainer = training.Trainer(updater, (epoch, 'epoch'))\n",
    "\n",
    "# 学習状況の表示や保存\n",
    "trainer.extend(extensions.LogReport())#ログ\n",
    "trainer.extend(extensions.Evaluator(test_iter, model))# エポック数の表示\n",
    "trainer.extend(extensions.PrintReport(['epoch', 'main/loss', 'validation/main/loss','main/accuracy', 'validation/main/accuracy', 'elapsed_time'] ))#計算状態の表示\n",
    "#trainer.extend(extensions.dump_graph('main/loss'))#ニューラルネットワークの構造\n",
    "#trainer.extend(extensions.PlotReport(['main/loss', 'validation/main/loss'], 'epoch',file_name='loss.png'))#誤差のグラフ\n",
    "#trainer.extend(extensions.PlotReport(['main/accuracy', 'validation/main/accuracy'],'epoch', file_name='accuracy.png'))#精度のグラフ\n",
    "#trainer.extend(extensions.snapshot(), trigger=(100, 'epoch'))# 学習再開のためのスナップショット出力\n",
    "#chainer.serializers.load_npz(\"result/snapshot_iter_500\", trainer)#再開用\n",
    "#chainer.serializers.save_npz(\"result/out.model\", model)\n",
    "\n",
    "# 学習開始\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  5. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ... 10.  0.  0.]\n",
      " [ 0.  0.  0. ... 16.  9.  0.]\n",
      " ...\n",
      " [ 0.  0.  1. ...  6.  0.  0.]\n",
      " [ 0.  0.  2. ... 12.  0.  0.]\n",
      " [ 0.  0. 10. ... 12.  1.  0.]]\n",
      "[0 1 2 ... 8 9 8]\n",
      "(1797, 64)\n",
      "[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3.\n",
      " 15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.\n",
      "  0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.\n",
      "  0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]\n",
      "[[[ 0.  0.  5. 13.  9.  1.  0.  0.]\n",
      "  [ 0.  0. 13. 15. 10. 15.  5.  0.]\n",
      "  [ 0.  3. 15.  2.  0. 11.  8.  0.]\n",
      "  [ 0.  4. 12.  0.  0.  8.  8.  0.]\n",
      "  [ 0.  5.  8.  0.  0.  9.  8.  0.]\n",
      "  [ 0.  4. 11.  0.  1. 12.  7.  0.]\n",
      "  [ 0.  2. 14.  5. 10. 12.  0.  0.]\n",
      "  [ 0.  0.  6. 13. 10.  0.  0.  0.]]]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "images_and_labels = list(zip(digits.images, digits.target))\n",
    "for index, (image, label) in enumerate(images_and_labels[:10]):\n",
    "    plt.subplot(2,5, index+1)\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title('training {}'.format(label))\n",
    "plt.show()\n",
    "\n",
    "print(digits.data)\n",
    "print(digits.target)\n",
    "print(digits.data.shape)\n",
    "print(digits.data[0])\n",
    "print(digits.data.reshape((len(digits.data), 1, 8,8))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyChain(chainer.Chain):\n",
    "    def __init_(self):\n",
    "        super(Mychain, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.conv1 = L.Convolution2D(1,16,3,1,1)\n",
    "            self.conv2 = L.Convolution2D(16,64,3,1,1)\n",
    "            self.l3 = L.Linear(256, 10)\n",
    "            \n",
    "    def __call__(self, x):\n",
    "        h1 = F.max_pooling_2d(F.relu(self.conv1(x)),2,2)\n",
    "        h2 = F.max_pooling_2d(F.relu(self.conv2(h2)), 2,2)\n",
    "        y = self.l3(h2)\n",
    "        return y\n",
    "    \n",
    "epoch=20\n",
    "batchsize=100\n",
    "digits = load_digits()\n",
    "data_train, data_test, label_train, label_test = train_test_split(digits.data, digits.target, test_size=0.2)\n",
    "data_train = data_train.reshape((len(data_train), 1, 8,8))\n",
    "data_test = data_test.reshape((len(data_test), 1, 8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
